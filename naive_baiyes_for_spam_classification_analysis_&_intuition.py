# -*- coding: utf-8 -*-
"""Naive Baiyes for Spam Classifier - Analysis & Intuition

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mr1EaxuADFB3k2LQMhFH2xWigms6b_96

## Naive Bayes for Spam Classification

In this section we will build a spam classificator using Naive Bayes probabilistic ML technique.

**Tip**: Remember, the first thing you will do when you receive some problem is transform it from business problem to ML problem. 
Example: Filter Spam >>> Take raw emails and pre-process text data. Then train a ML model that classifies the email as either spam or not-spam.

In ML we commonly have two types of problems:

* **Regressions**: You need to predict a number (time, value, amount etc.) 

* **Classification:** You need to classify something (Cancer or Not Cancer, Spam or Not Spam etc.)

For e-mail classification we have an additional challenge! We will need to adapt the body of our e-mail (text) to numbers (understandable for our algorithm).

## Naive Bayes Classifier

The main good attributes of this model are Simplicity and Speed! Spam classification and Weather forecasting are classic applications.

**How it works:** To make a decision the naive bayes classify compares probabilities (the likelihood of some event). Translating to our problem, the algorithm will calculate the probability to be or not to be spam. In addition, we will have a chart with two probabilities Spam (x-axis) or Not Spam (y-axis). The 50% / 50% probability will create a straight line called **Decision Boundary**.

## A little review about probability

**1. Basic Probability**

Normally in probability problems we will have a division between a small group by a large group. Example: What is the probability to be hit by a ligthning strike? The equation is the amount of people hit by lightning strikes / total amount of lightning strikes (240.000 / 350 MM) ~ 0.07%.

In our context, What is the probability of an email being spam? Nr Spam Emails Sent / Total Nr Emails Sent >> 148bn / 269bn = 55%!!


**2. Joint Proability**

If you flip coin twice, what is the proability to get heads 2x? This is the join probability! when you have additional recurrence. The answer for our problem is 1 / 4 ~ 25%, because we have 2x 50% of chances (50% / 2). Likewise, If you establish a confusion matrix, you will see that heads / heads is only one change in four. Matematically, multiply the probability of getting heads by itself! p(A) x p(B) 

**Important**: Each time you toss a dice or flip a coin you will have the same probability! Imagine the following situation; you fliped a coin twice and both times you got head. What is the probability to get head again? 50%! The probability are independent by itself!

**3. Conditional Probability**

This is the hard topic in the probability, here we will have dependency of something to calculate the probability of another event. Given our spam email problem, a spam email is composed by a certain pattern of words and features. But, given the word 'discount' in an email, is this email a spam or not? Measures the probability of some event occur given a previous event has occured. 

For example, the weather is cloudy, what is the probability of raining today? Given that the day is cloudy, what is the probability of raining?


Mathematically:

p(Rain / Cloudy) = p(Rain and Cloudy) / p(Cloudy)

This sounds fundamental of machine learning right? learn something to aswer another thing. Knowing the move budget we can calculate the revenue, Knowing the number of rooms we can calculate the expected price.

The problem here is to calculate the upper probability, when we have dependent actions (independent like toss a dice is easy). To calculate his probability for dependent problems we will use the Bayes Theorem.

## Bayes Theorem

Given the difficult of our conditional probability, we can reshuffle our equation for an easier version. So, let's change the question for our cloud x raining problem.

Given that is raining what is the probability of being cloudy? Mathematically would be: p(Rain / Cloudy) = p(Cloudy / Rain) p(Rain) / p(Cloudy).

Let's get back to our Email Spam problem:

* p(Spam / Viagra) = p(Viagra / Spam) p(Spam) / p(Viagra)
* p(Spam / Viagra) = (65/370.000 >> Check our dataset all emails with viagra and in the spam / all spams) * 0.55% / (75/700.000 >> Google the basic prob) 

In an email we can calculate for every single word the probability of being in an ordinary spam email. We can also use NPL to make our job easier, because some words are more frequent in spam emails.

Let's give another example: given that email has the word 'free' and 'viagra'?
We will the joint probabilities of conditional probabilities like we have seen previously, because the events are independent! 

**The Naive from Naive Bayes assumes independency of each word, that's the trick in our algorithm!**

So, image an email with the following body: "Hello friend, Want free viagra?" we will calculate the joint probabilities from condition probabilities in each word of being spam and not being spam to plot in our graph!

p(Spam / Hello) * p(Spam / Want) * p(Spam / Free) * p(Spam / Viagra)

p(Normal / Hello) * p(Normal / Want) * p(Normal / Free) * p(Normal / Viagra)

We will use the **bag of words** technique: each word is looked isoletedly, removing dependency of words like (New York or Bad Idea).
"""

# Import libraries and creating constants
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys

from os import walk
from os.path import join
from sklearn.model_selection import train_test_split
from bs4 import BeautifulSoup ## Library that is used for Webscrapping and RPA. It is helpful for treating HTML

# Text Pre-processing Package - NLTK 
import nltk
from nltk.stem import PorterStemmer # Cambridge University - English Language
from nltk.stem import SnowballStemmer # For other languages 
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Constants for relative file path
# For Google Colaboratory you need to upload this files on the cloud (Google Drive)
# If you are using cloud storage (Google Drive) you don't need to use 'r' as a file path

RELATIVE_PATH_EXAMPLE = '/content/drive/My Drive/SpamData/01_Processing/practice_email.txt'  
SPAM_1_PATH = '/content/drive/My Drive/SpamData/01_Processing/spam_assassin_corpus/spam_1'
SPAM_2_PATH = '/content/drive/My Drive/SpamData/01_Processing/spam_assassin_corpus/spam_2'
EASY_NONSPAM_1_PATH = '/content/drive/My Drive/SpamData/01_Processing/spam_assassin_corpus/easy_ham_1'
EASY_NONSPAM_2_PATH = '/content/drive/My Drive/SpamData/01_Processing/spam_assassin_corpus/easy_ham_2'

SPAM_CATEGORY = 1
HAM_CATEGORY = 0
VOCAB_SIZE = 2500

DATA_JSON_FILE = '/content/drive/My Drive/SpamData/01_Processing/email-text-data.json'
WORD_ID_FILE = '/content/drive/My Drive/SpamData/01_Processing/word-by-id.csv'

TRAINING_DATA_FILE ='/content/drive/My Drive/SpamData/02_Training/train-data.txt'
TEST_DATA_FILE = '/content/drive/My Drive/SpamData/02_Training/test-data.txt'

WHALE_FILE = r'SpamData\01_Processing\wordcloud_resources\whale-icon.png'
THUMBS_UP_FILE= r'SpamData\01_Processing\wordcloud_resources\thumbs-up.png'
THUMBS_DOWN_FILE= r'SpamData\01_Processing\wordcloud_resources\thumbs-down.png'

"""## **Step 1: Reading our data from files**

Our data is structured in two folders. One contains files with spam e-mails, and the other contains files from regular emails (as known as ham). Thus, we need to create a structure to opean each file, read and save into a string or list.

Data preparation steps: 

  1. Read all e-mails and extract the e-mail body
  2. Create a pandas DataFrame with classification Spam / Not Spam for all e-mails

First let's start with some examples:

* Example 1: Reading one e-mail as a sample. (Raw data)
* Example 2: Extracting the e-mail body. (This will be the data for our model)
"""

# Here is an example from an e-mail that will be part of our analysis
stream = open(RELATIVE_PATH_EXAMPLE, encoding='latin-1')
message= stream.read()
stream.close()

print(message)

# Extracting the email body from raw data
stream = open(RELATIVE_PATH_EXAMPLE, encoding='latin-1') # Open buffer for file reading. OBS: UTF-8 encoding by default. You can change if necessary

is_body = False
lines = []

for line in stream: # Loops through the file

    if is_body: # If is in the body let's append into our list
    
        lines.append(line)

    elif line == '\n': # In our email the body starts with a blank space 

        is_body = True
        
stream.close() # close the buffer

# Transform our list into a simple string by using 'Join'!
email_body = '\n'.join(lines) 

print(email_body)

"""### Extra Topic: Generator functions & Yield keyword

Before we start pre-processing our e-mails (extract the information from the e-mail body and make adjustements / analysis), let's take a look in another way to process massively multiple files.

**Generator functions:** This type of function creates an interator that controls automatically all the work of reading and indexing, lists, function calls, file processing etc. It's like if we have a 'memory effect' controller, which allows us gain performance while processing a massive amount of data without affecting directly our memory.

Instead the word **'return' we will use the keyword 'yield'** that returns a lazy iterator, like a pointer in C++ preventing memory errors.

This special type of function is commonly used for file processing!

Let's learn with an example:
"""

def generate_squares(n):

  for my_number in range(n): # elevate the number 'n' times as a sequence

    yield my_number ** 2 # we don't have return, instead we have yield that acts like a memory from were we stopped

# Generator function call

for i in generate_squares(10):
    
    print(i, end='->')

# n = 5 the exit will be: 0->1->4->9->16->
# n = 10 the exit will be: 0->1->4->9->16->25->36->49->64->81->

"""### Extracing our e-mail body from files

Now, We have learnt about generator functions. Let's create a function that extracts all the e-mail body from our text files.
"""

def email_body_generator(path):

  # 1. Iterates through the file path
  # 'walk' function literally walks through the files one by one returning a tuple of informations about file location
  for root, dirnames, filenames in walk(path): 

    # 2. for each file name
    for filename in filenames:

      # Joins the root path + file name to create a full file path
      filepath = join(root, filename)

      stream = open(filepath, mode='r', encoding='latin-1') # Open file buffer for reading ('r'), encoded by latin-1

      is_body = False # Controls if is a body
      lines = [] # List that will save our entire body

      for line in stream:

        if is_body:

          lines.append(line)

        elif line == '\n': # '\n indicates that is the end of HTML header

          is_body = True

      stream.close() # Close Buffer
      email_body = '\n'.join(lines) # Join our lines in a full body object

      yield filename, email_body

"""### Create pandas DataFrame with classification

Now we will create a Data Frame with a classification 


assign a classification for our emails by using the following assumption:

* 1 > Spam
* 0 > Not Spam (Commonly assigned as Ham)
"""

def df_from_directory(path, classification):

  rows = []
  row_names = []

  for file_name, email_body in email_body_generator(path):

    rows.append({'Message': email_body, 'Category': classification})
    row_names.append(file_name)

  return pd.DataFrame(rows, index=row_names)

# Build our Spam and Non Spam Dataframes
spam_emails = df_from_directory(SPAM_1_PATH, SPAM_CATEGORY)
spam_emails = spam_emails.append(df_from_directory(SPAM_2_PATH, SPAM_CATEGORY))

ham_emails = df_from_directory(EASY_NONSPAM_1_PATH, HAM_CATEGORY)
ham_emails = ham_emails.append(df_from_directory(EASY_NONSPAM_2_PATH, HAM_CATEGORY))

# Check the shape from DataFrames
print('Spam e-mails shape:', spam_emails.shape)
print('Spam e-mails shape:', ham_emails.shape)

# Concatenate Spam + Not Spam emails in one unique pandas DataFrame
data = pd.concat([spam_emails, ham_emails])
print('Full DataFrame shape:', data.shape)

data.head()

"""## **Step 2: Cleaning our data and adding an email tracker in the Dataset**

In this step we will extract relevant e-mail bodies and check the quality of our data. In addition, we will set up a new numerical index to our data set. Currently, the file name is our unique id from our dataset, and is not trivial manipulate data with a hashed ID. For this reason a new numerical index will be helpful in order to track e-mails later on!

Steps to follow:

1. Check 'null', NaN, None, or missing values
2. Check for empty emails
3. Add document index to track emails in the Dataset
"""

# 1. Checking Null / None --> Do our emails have null values on their bodies?

# .isnull >> returns True/False if some null value is found
# .values >> get the values and not the boolean result
# .any() >> sum up the previous sentence for all DF. 
data['Message'].isnull().values.any() 

# Result: False --> Our DataFrame does not contain any Null/None value! In other words we don't have empty emails

# Double check --> must return 0 given 'False' statement
data.Message.isnull().sum()

# 2. Checking empty emails --> Do our emails are blank?

# The first () will return True or False, and then the .any() will sum up all records that satisfy the sentence in a single answer
(data.Message.str.len() == 0).any()
# Result: True --> There are blank e-mails!


# Check the amount of empty emails
(data.Message.str.len() == 0).sum()
# Result: 3 emails

# Checking the index from these 3 empty emails
data[data.Message.str.len() == 0].index # Return the index from the empty emails
# Result: Index(['cmds', 'cmds', 'cmds'], dtype='object') ---> cmds is a system file when you unzip emails. So let's remove!


# Drop the empty emails rows and overwrites our Data Frame 
data.drop(['cmds'], inplace=True) #inplace=True >> Overwrite

# Validation: Checking again the sum of empty emails
(data.Message.str.len() == 0).sum()
# Result: 0 --> OK

# 3. Add document index to track emails in the Dataset

# Create our index with the same length from our Dataset
document_ids = range(0, len(data.index))

# Apply to our dataset
data['Doc_ID'] = document_ids

# Redirect our file name as a new column before dropping as a index
data['File_Name'] = data.index

# Drop the previous index (file name witouth any column name), and resettle a new one (Doc_ID)
data.set_index('Doc_ID', inplace=True) # inplace = overwritting

data.head()

"""### Extra Topic: Saving our data JSON file with Pandas

This is really helpful when you need to interact with other platforms. Specially if your model will run with a Web Application!
"""

data.to_json(DATA_JSON_FILE)

"""## **Step 3: Data Visualisation**

Let's visualize our data and get a comprehensive knowledge about!
"""

# Counts the value given the column 'Category'. This will return an array
data.Category.value_counts()

# Spam = 1.896
# Not Spam = 3.900

amount_spam = data.Category.value_counts()[1] 
amount_not_spam = data.Category.value_counts()[0] 

print('Amount Spam:', amount_spam)
print('Amount Not Spam:', amount_not_spam)

# Pie / Donut Chart Plotting

# chart parameters
category_names = ['Spam', 'Legit Mail']
sizes = [amount_spam, amount_not_spam]
custom_colours = ['#D63447','#DAE1E7']


plt.figure(figsize=(2, 2), dpi=227) # 227 density of pixels per inch
plt.pie(sizes, 
        labels=category_names, 
        textprops={'fontsize': 6}, 
        startangle=90, autopct='%1.2f%%', 
        colors=custom_colours
        #explode=[0, 0.1] >> this offsert separe our pie!
       ) # startangle --> Rotate our start angle for better visualization / autopct --> add automatically % / 

# Let's draw a Donut chart given our Pie Chart!
centre_circle = plt.Circle((0,0), radius= 0.6, fc='white') #xy tuple coordinate / radius / colour
plt.gca().add_artist(centre_circle)

plt.show()

"""## **Step 4: NPL - Natural Language Processing**

NPL is often used for Google Search, Personal Assistants (Siri, Alexia), Sentiment Analysis, Tweets, Google Ad Words, Autocorrection, Spell check, Translating...

NLTK is a python package that will help us with many functions for natural language processing.

We will follow the steps below to pre-process our data:

1. **Lower Case treatment**: Forcefully converts all string to lower case and prevent user incorrect inputs.
2. **Tokenising**: Split words individually
3. **Stop Words removal**: Exclude articles, conjunctions etc. (Example: 'the', 'to')
4. **Stemming**: Transforms a word to it's stem format (Example: Going / Goes = Go)
5. **Punctuation treatment**: Exclude commas, exclamation, periods etc.
6. **HTML tags removal**: Given our scenario, some e-mails may contain HTML tags that will cause noise for our data

**Notice**: This steps are used for ML purposes. Depending on what is your problem you will need another strategy!

Let's start with examples from our steps
"""

msg = 'All work And no PlaY makes JacK a DULL BOY'

nltk.download('punkt') # Download multiple tokenizer tools in a folder 

# Transforming to lower case
word_tokenize(msg.lower())

# Removing Stop Words >> Conjunctions, Linkings 
# Example: which, at, on, in, the, what etc... This is good for grammar but it's not useful for ML

nltk.download('stopwords') # Download the stop word tools for multiple languages and strategies.

# Set data structure: This data structure will be helpful in this type of problem, given it's efficiency on searching!
# Set >> It is an ordered, unidexed set of items!

# Load english stop words in a set
stop_words = set(stopwords.words('english'))

# Example >> Check the stop word
if 'this' in stop_words:
    
    print('found!')

# Lowering Cases + Tokenising + Removing stop words

msg = 'All work and no Play makes Jack a DULL BOY.'

words = word_tokenize(msg.lower()) # tokenising and lowering case

stop_words = set(stopwords.words('english')) # load english stop words in a set

filtered_words = [] # Saves the list without stop words

for word in words: # for each word in our message..
    
    if word not in stop_words: # Check if the word is not a stop word...
        
        filtered_words.append(word) # Append the result

print(filtered_words)

# Word Stemming + Removing Punctuations

# Stemming >> Transform the word into its natural form. (Goes / Going >> Go)

# Porter Stemmer >> Default stemmer algorithm for English Language
# SnowballStemmer >> Another version for stemming with good efficiency!

# Removing Punctuation >> There is a native function called .isalpha() that returns False if it is a punctuation mark.


# Let's apply stemming + punctuations treatment on our previous NPL techniques

#stemmer = PorterStemmer() --> By default is english
stemmer = SnowballStemmer('english') # English language stemmer

msg = 'All work and no play makes Jack a dull boy. To be or Not to Be. \
        Nobody expects the Spanish Inquisition'

words = word_tokenize(msg.lower()) # tokenising and lowering case

stop_words = set(stopwords.words('english')) # load english stop words in a set

filtered_words = [] # Saves the list without stop words

for word in words: # for each word in our message..
    
    if word not in stop_words and word.isalpha(): # Check if the word is not a stop word and remove punctuation...

        stemmed_word = stemmer.stem(word)
        filtered_words.append(stemmed_word) # Append the result

print(filtered_words) # With Stemmer! ['work', 'play', 'make', 'jack', 'dull', 'boy', '.']

data.Message

# Removing HTML Tags from email body

# We will use the BeautifulSoup html.parser as a controller to remove our HTML tags
soup = BeautifulSoup(data.at[2,'Message'], 'html.parser') # Text we would like to pass, Our parser

print(soup.prettify()) # Brings the message in a better way

# Removing HTML tags by getting only text!
soup.get_text()

"""Now, let's apply all these techniques to our Data Frame!"""

# NPL function for e-mail text pre-processing 
def clean_message(message, stemmer=PorterStemmer(), stop_words= set(stopwords.words('english'))):

  filtered_words = [] # our final list

  # 1. Remove HTML Tags
  soup = BeautifulSoup(message,'html.parser')
  cleaned_text = soup.get_text()

  # 2. Tokenize and convert to lower case
  words = word_tokenize(cleaned_text.lower())

  # 3. Remove stop words, punctuations and Stemming the word list for each word in our message
  # for each word in our message
  for word in words:

    # Remove stop words and punctuations
    if word not in stop_words and word.isalpha():

      # Stemming our word + Append in our final list
      filtered_words.append(stemmer.stem(word))

  return filtered_words

"""Now let's apply these NPL techniques to our email dataframe

Before we start, we will learn **how to slice dataframes and series & create subsets** in order to make our work easier.
"""

# Slicing dataframes and series & Creating subsets

# Slicing >> .iloc[start:stop] Selects more than 1 row sequentially
data.iloc[0:2] # Select the first two rows (0 to 2)
data.iloc[5:11] # Select the rows from 5 to 11 (5 to 10)

first_emails = data.Message.iloc[0:3] # You can specify the column

# Apply function >> apply some function to a dataset. In this case we are applying clean_message function to first_emails dataset
nested_list = first_emails.apply(clean_message)

nested_list

# Generating all emails in a single list (flattened list)

# Solution 1: Traditional form (2x loops + append)
#for sublist in nested_list:
#  for item in sublist:
#    flat_list.append(item)

# Solution 2: Using list comprehension syntax (1 line of code, not so readable)
flat_list = [item for sublist in nested_list for item in sublist]

# Apply our NPL functions to our Dataframe
messages_list = data.Message.apply(clean_message)

# Slicing data frames using logic

# We will create a new dataset with the condition into the []
docs_id_spam = data[data.Category == 1].index
docs_id_ham = data[data.Category == 0].index


# We can retrieved specific indexes with the 'loc' function
nested_list_spam = messages_list.loc[docs_id_spam]
nested_list_ham = messages_list.loc[docs_id_ham]

print('Nested list spam shape:',nested_list_spam.shape)
print('Nested list ham shape: ', nested_list_ham.shape)

"""Now, Let's check which are the most common words in our dataset. This will be helpful in order to select the most frequent words for spam and not spam emails"""

# Checking the most common words in our NOT spam set of words
flat_list_ham = [item for sublist in nested_list_ham for item in sublist] # retrieve all data
normal_words = pd.Series(flat_list_ham).value_counts() # Create a Panda Series with our list and remove duplicates with value_counts

normal_words.shape[0] # total number of words witout duplications >> 20.815

normal_words[:10]

# Checking the most common words in our spam set of words
flat_list_spam = [item for sublist in nested_list_spam for item in sublist] # retrieve all data
spam_words = pd.Series(flat_list_spam).value_counts() # Create a Panda Series with our list and remove duplicates with value_counts

print(spam_words.shape[0]) # total number of words witout duplications >> 13.242

spam_words[:10]

"""## **Step 5: Creating the vocabulary and dictionary for our spam classifier**

Finishing our text pre-processing. There are lots of individual words among the 5.800 odd emails our dataset. We won't use every single word from the email body, we will use an amount of most frequent words (2.500 in our case).
"""

# Generating the Vocabulary from our stemmed words
stemmed_nested_list = data.Message.apply(clean_message)

# Flatten the stemmed list 
flat_stemmed_list = [item for sublist in stemmed_nested_list for item in sublist]

# Get the unique list of words
unique_words = pd.Series(flat_stemmed_list).value_counts()

print('Nr of unique words:', unique_words.shape[0]) 

unique_words.head()

frequent_words = unique_words[0:VOCAB_SIZE] # Get the most 2.500 frequent words [begin:end] if not declared is = 0
print('Most common 10 words: \n', frequent_words[:10])

# Creating a vocabulary Data Frame with a WORD_ID

word_ids = list(range(0,VOCAB_SIZE))

vocabulary = pd.DataFrame({'Vocab_Words': frequent_words.index.values}, index= word_ids) #index.values >> get the values from the Series
vocabulary.index.name = 'Word_ID'

vocabulary.head()

# Saving our word list as CSV
vocabulary.to_csv(WORD_ID_FILE, index_label=vocabulary.index.name, header=vocabulary.Vocab_Words.name)

"""Practicing the techniques we have learnt"""

## Exercise 1: Check if a word is part of the vocabulary

any(vocabulary.Vocab_Words == 'http') # inefficient

# Convert our Data frame to a set to become more efficient!
print('http' in set(vocabulary.Vocab_Words))


## Exercise 2: Print out the number of words in the longest email (after cleaning & stemming)
# Note the longest email's position in the list of cleaned emails. Print out the stemmed list of the words from the data frame

clean_email_lengths = [] # number of character in each email

for sublist in stemmed_nested_list:
    
    clean_email_lengths.append(len(sublist))
    
# Alternatively with list comprehension
clean_email_lengths = [len(sublist) for sublist in stemmed_nested_list]

print('Number of words in the longest email: ', max(clean_email_lengths))
print('Email position in the list and the dataframe: ', np.argmax(clean_email_lengths)) # Returns the location in the largest value

stemmed_nested_list[np.argmax(clean_email_lengths)] # Collects the index from the biggest value

"""## **Step 6: Sparse Matrix** 

This is an excellend data structure for Machine Learning data frames

With this sort of Matrix we will exclude zeroed data. In our example, we will use words that occurs in spam / not spam emails, acting like a compressed version

In our example, each word will be placed with a label if it is spam or not and some level of occurence, and some words are zeros.
"""

type(stemmed_nested_list[2]) # It is a DF with multiple lists in each index.

word_columns_df = pd.DataFrame.from_records(stemmed_nested_list.to_list()) # Transforming each word in a column as individual data point
word_columns_df.head()

# Df Shape: 5796 (emails that we have) x 7671 (number of words in the longest email)

# Splitting the data into a training and testing data set

# Important: We will shuffle our DF in order to create this subsets
# Important 2: Try to use Validation + Test sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(word_columns_df, data.Category, test_size= 0.3, random_state= 42) # random state = seed value

X_train.index.name = X_test.index.name = 'DOC_ID'
X_train.head()

y_train.head() # Note that our DOC_ID matches between X and y datasets

# Creating the Sparse Matrix

# Create a particular index given a  DataFrame
word_index = pd.Index(vocabulary.Vocab_Words)

type(word_index)

# Let's create our function to process

def create_sparse_matrix(df, indexed_words, labels): # Data Frame, Index Words + y values
    
    """
    Returns spare matrix as dataframe
    
    df: A dataframe with words in the columns with a document id (Doc_Id) as an index (X_train or X_test).
    indexed_words
    
    indexed_words: Index of words by word id (Word_Id)
    
    labels: Category as a series (y_train or y_test).
    
    """
    
    # Control variables
    nr_rows = df.shape[0]
    nr_columns = df.shape[1]
    word_set = set(indexed_words) # create a python set from our indexed_words
    dict_list = [] # Creates an empty dictionary
    
    # Loop through df
    for i in range(nr_rows):
        for j in range(nr_columns):
            
            word = df.iat[i , j]
            
            if word in word_set:
                
                doc_id = df.index[i] # Get the Doc_Id
                word_id = indexed_words.get_loc(word) # Get the Word_Id
                category = labels.at[doc_id] # Get the category (Spam / Not Spam)
                
                # Create the item dictionary and append to our list
                item= {'LABEL': category, 
                       'DOC_ID': doc_id, 
                       'OCCURRENCE': 1, 
                       'WORD_ID': word_id}
                
                dict_list.append(item)
            
    
    return pd.DataFrame(dict_list)

# Let's run our function
sparse_train_df = create_sparse_matrix(X_train, word_index, y_train)

# If we take a peek at our Sparse Data Frame we will find that if we have the same word in the email, it is duplicated. So we need to group by 
sparse_train_df[-5:]  # last 5 rows

# Combining occurencies with the Pandas groupby() method
train_grouped = sparse_train_df.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum() # Group by that keys and sum up the occurencies

train_grouped.head()

# Will make our doc id for every single row and not summarized
train_grouped = train_grouped.reset_index()
train_grouped.head()

# Saving our work (as .txt)
np.savetxt(TRAINING_DATA_FILE, train_grouped, fmt='%d') # Relative Path , Data to be saved, and format

# Creating a spare matrix for our test dataframe

# Create the sparse matrix
sparse_test_df = create_sparse_matrix(X_test, word_index, y_test)

# Group our data by 3 columns summing up our Occurencies
test_grouped = sparse_test_df.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum()

# Add the Doc_Id for each row (Reseting the index)
test_grouped = train_grouped.reset_index()

# Saving our grouped test (as .txt)
np.savetxt(TEST_DATA_FILE, test_grouped, fmt='%d') # Relative Path , Data to be saved, and format

# Checking our text pre-processing

# We started with 5.796 emails, and then splitted in 4.057 and 1.739 for testing. Are the number equal to our .txt?
# How many individual emails were included in the testing file?

# Check our Document IDs 
train_doc_ids = set(train_grouped.DOC_ID) # 4.014 vs 4.057 
test_doc_ids = set(test_grouped.DOC_ID) # 1.723  vs 1.739

# Why some emails were excluded? Which were the emails?

# 1st let's compare the sets
set(X_test.index.values) - test_doc_ids

"""## Extra Topic: Word Cloud charts

It's not scientific but it's beautiful! and grabs people's attention
"""